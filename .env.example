# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Available providers: "ollama" | "openai_compat" | "openai"
LLM_PROVIDER="ollama"

# =============================================================================
# Ollama Settings (LLM_PROVIDER="ollama")
# =============================================================================
OLLAMA_MODEL="qwen3:4b"
OLLAMA_BASE_URL="http://localhost:11434"

# =============================================================================
# OpenAI-Compatible Server Settings (LLM_PROVIDER="openai_compat")
# For vLLM, LocalAI, or other OpenAI-compatible servers
# =============================================================================
OPENAI_COMPAT_BASE_URL="http://localhost:8000/v1"
OPENAI_COMPAT_API_KEY="NONEEDKEY"
OPENAI_COMPAT_MODEL="Qwen/Qwen2.5-7B-Instruct"

# =============================================================================
# Native OpenAI API Settings (LLM_PROVIDER="openai")
# For GPT-4o, GPT-4-turbo, etc.
# =============================================================================
OPENAI_API_KEY="sk-..."
OPENAI_MODEL="gpt-4o"

# =============================================================================
# Common LLM Parameters
# =============================================================================
TEMPERATURE=0.3
MAX_TOKENS=4096
USE_STRUCTURED_OUTPUT=false

# =============================================================================
# Review Bot Settings
# =============================================================================
REVIEW_DEFAULT_VARIANT=G0-baseline
REVIEW_PACKS_DIR=backend/domain/prompts/packs
REVIEW_PRESETS_DIR=backend/pipelines/presets
REVIEW_ALLOWED_VARIANTS_RAW=G0-baseline,g1-mapreduce,g2-iterative,g3-multipersona,g4-multireview
# REVIEW_REPO_PATH=/path/to/target/repo
